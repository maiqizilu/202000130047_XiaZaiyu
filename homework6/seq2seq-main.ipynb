{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入实验所需的第三方模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import unicodedata\n",
    "import math\n",
    "\n",
    "from mindspore import Tensor, nn, Model, context\n",
    "from mindspore.train.serialization import load_param_into_net, load_checkpoint\n",
    "from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore import Parameter\n",
    "from mindspore.nn.loss.loss import _Loss\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common import dtype as mstype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.设置运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.查看数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\t嗨。\n",
      "\n",
      "Hi.\t你好。\n",
      "\n",
      "Run.\t你用跑的。\n",
      "\n",
      "Wait!\t等等！\n",
      "\n",
      "Hello!\t你好。\n",
      "\n",
      "I try.\t让我来。\n",
      "\n",
      "I won!\t我赢了。\n",
      "\n",
      "Oh no!\t不会吧。\n",
      "\n",
      "Cheers!\t干杯!\n",
      "\n",
      "He ran.\t他跑了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看训练数据内容前10行内容\n",
    "with open(\"cmn_zhsim.txt\", 'r', encoding='utf-8') as f:\n",
    "        for i in range(10):\n",
    "            print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = \"<eos>\"\n",
    "SOS = \"<sos>\"\n",
    "MAX_SEQ_LEN=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们需要将字符转化为ASCII编码\n",
    "#并全部转化为小写字母，并修剪大部分标点符号\n",
    "#除了(a-z, A-Z, \".\", \"?\", \"!\", \",\")这些字符外，全替换成空格\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = unicodeToAscii(s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_path, vocab_save_path, max_seq_len):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # 读取文本文件，按行分割，再将每行分割成语句对\n",
    "    data = data.split('\\n')\n",
    "\n",
    "     # 截取前2000行数据进行训练\n",
    "    data = data[:2000]\n",
    "\n",
    "    # 分割每行中的中英文\n",
    "    en_data = [normalizeString(line.split('\\t')[0]) for line in data]\n",
    "\n",
    "    ch_data = [line.split('\\t')[1] for line in data]\n",
    "\n",
    "    # 利用集合，获得中英文词汇表\n",
    "    en_vocab = set(' '.join(en_data).split(' '))\n",
    "    id2en = [EOS] + [SOS] + list(en_vocab)\n",
    "    en2id = {c:i for i,c in enumerate(id2en)}\n",
    "    en_vocab_size = len(id2en)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "\n",
    "    ch_vocab = set(''.join(ch_data))\n",
    "    id2ch = [EOS] + [SOS] + list(ch_vocab)\n",
    "    ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "    ch_vocab_size = len(id2ch)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    # 将句子用词汇表id表示\n",
    "    en_num_data = np.array([[1] + [int(en2id[en]) for en in line.split(' ')] + [0] for line in en_data])\n",
    "    ch_num_data = np.array([[1] + [int(ch2id[ch]) for ch in line] + [0] for line in ch_data])\n",
    "\n",
    "    #将短句子扩充到统一的长度\n",
    "    for i in range(len(en_num_data)):\n",
    "        num = max_seq_len + 1 - len(en_num_data[i])\n",
    "        if(num >= 0):\n",
    "            en_num_data[i] += [0]*num\n",
    "        else:\n",
    "            en_num_data[i] = en_num_data[i][:max_seq_len] + [0]\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        num = max_seq_len + 1 - len(ch_num_data[i])\n",
    "        if(num >= 0):\n",
    "            ch_num_data[i] += [0]*num\n",
    "        else:\n",
    "            ch_num_data[i] = ch_num_data[i][:max_seq_len] + [0]\n",
    "    \n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    return en_num_data, ch_num_data, en_vocab_size, ch_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.获得mindrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将处理后的数据保存为mindrecord文件，方便后续训练\n",
    "def convert_to_mindrecord(data_path, mindrecord_save_path, max_seq_len):\n",
    "    en_num_data, ch_num_data, en_vocab_size, ch_vocab_size = prepare_data(data_path, mindrecord_save_path, max_seq_len)\n",
    "\n",
    "    # 输出前十行英文句子对应的数据\n",
    "    for i in range(10):\n",
    "        print(en_num_data[i])\n",
    "    \n",
    "    data_list_train = []\n",
    "    for en, ch in zip(en_num_data, ch_num_data):\n",
    "        en = np.array(en).astype(np.int32)\n",
    "        ch = np.array(ch).astype(np.int32)\n",
    "        data_json = {\"encoder_data\": en.reshape(-1),\n",
    "                     \"decoder_data\": ch.reshape(-1)}\n",
    "        data_list_train.append(data_json)\n",
    "    \n",
    "    data_list_eval = random.sample(data_list_train, 20)\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_train.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    schema_json = {\"encoder_data\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"decoder_data\": {\"type\": \"int32\", \"shape\": [-1]}}\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_train)\n",
    "    writer.commit()\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_eval.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_eval)\n",
    "    writer.commit()\n",
    "\n",
    "    # 输出英文词表大小\n",
    "    print(\"en_vocab_size: \", en_vocab_size)\n",
    "    # 输出中文词表大小\n",
    "    print(\"ch_vocab_size: \", ch_vocab_size)\n",
    "\n",
    "    return en_vocab_size, ch_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 734, 764, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 734, 764, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 828, 764, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 544, 735, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 217, 735, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 137, 1100, 764, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 137, 721, 735, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1071, 59, 735, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1096, 735, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 792, 512, 764, 0, 0, 0, 0, 0, 0, 0]\n",
      "en_vocab_size:  1154\n",
      "ch_vocab_size:  1116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1154, 1116)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用convert_to_mindrecord方法，输出前十行英文句子对应的数据\n",
    "# 注意！！！由于工作空间中已经存在preprocess文件夹及mindrecord文件，第二次运行convert_to_mindrecord方法时会报错。\n",
    "# 此时请运行下面注释中的linux命令删除preprocess文件夹\n",
    "!rm -rf preprocess/\n",
    "if not os.path.exists(\"./preprocess\"):\n",
    "    os.mkdir('./preprocess')\n",
    "convert_to_mindrecord(\"cmn_zhsim.txt\", './preprocess', MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt\t       cmn_zhsim_mini.txt  preprocess  seq2seq-main-zss2.ipynb\n",
      "cmn_zhsim.txt  kernel_meta\t   rank_0      seq2seq-main_lihao.ipynb\n"
     ]
    }
   ],
   "source": [
    "# 运行linux命令，查看当前路径下的文件，可以看到，在工作空间中，存在cmn_zhsim.txt数据集文件，和preprocess文件夹。\n",
    "# 注意！！！preprocess文件夹为当前代码创建，只存在于当前的工作空间，并不在OBS桶中。\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "# hidden_size默认为1024, num_epochs默认为15\n",
    "#注意！！！'en_vocab_size'与'ch_vocab_size'两项，使用cmn_zhsim.txt和cmn_zhsim_mini.txt会有不同的设置，具体值请查看步骤5\n",
    "\n",
    "# CONFIG\n",
    "cfg = edict({\n",
    "    'en_vocab_size': 1154,\n",
    "    'ch_vocab_size': 1116,\n",
    "    'max_seq_length': 10,\n",
    "    'hidden_size': 1536,\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'num_epochs': 50,\n",
    "    'save_checkpoint_steps': 125,\n",
    "    'keep_checkpoint_max': 10,\n",
    "    'dataset_path':'./preprocess',\n",
    "    'ckpt_save_path':'./ckpt',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.定义读取mindrecord函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    target_data = decoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data, target_data\n",
    "\n",
    "def eval_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data\n",
    "\n",
    "def create_dataset(data_home, batch_size, repeat_num=1, is_training=True, device_num=1, rank=0):\n",
    "    if is_training:\n",
    "        data_dir = os.path.join(data_home, \"gru_train.mindrecord\")\n",
    "    else:\n",
    "        data_dir = os.path.join(data_home, \"gru_eval.mindrecord\")\n",
    "    data_set = ds.MindDataset(data_dir, columns_list=[\"encoder_data\",\"decoder_data\"], \n",
    "                              num_parallel_workers=4,\n",
    "                              num_shards=device_num, shard_id=rank)\n",
    "    if is_training:\n",
    "        operations = target_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                    output_columns=[\"encoder_data\",\"decoder_data\",\"target_data\"],\n",
    "                    column_order=[\"encoder_data\",\"decoder_data\",\"target_data\"])\n",
    "    else:\n",
    "        operations = eval_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                   output_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                   column_order=[\"encoder_data\",\"decoder_data\"])\n",
    "    data_set = data_set.shuffle(buffer_size=data_set.get_dataset_size())\n",
    "    data_set = data_set.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    data_set = data_set.repeat(count=repeat_num)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = create_dataset(cfg.dataset_path, cfg.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.定义GRU单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "    '''Weight init for gru cell'''\n",
    "    stdv = 1 / math.sqrt(hidden_size)\n",
    "    weight_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n",
    "                         name='weight_i')\n",
    "    weight_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n",
    "                         name='weight_h')\n",
    "    bias_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n",
    "    bias_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n",
    "    return weight_i, weight_h, bias_i, bias_h\n",
    "\n",
    "'''\n",
    "定义GRU单元，查阅官方文档，了解P.DynamicGRUV2()的输入输出组成与计算原理，并写入实验报告。\n",
    "'''\n",
    "class GRU(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(GRU, self).__init__()\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n",
    "            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n",
    "        self.rnn = P.DynamicGRUV2()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        x = self.cast(x, mstype.float16)\n",
    "        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n",
    "        return y1, h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.定义Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = config.en_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n",
    "\n",
    "    def construct(self, encoder_input):\n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.embedding，对输入encoder_input进行Embedding编码。\n",
    "        并根据self.perm定义的维度，通过self.trans进行维度变换。\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出命名为embeddings\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        out = self.embedding(encoder_input)\n",
    "        embeddings = self.trans(out,self.perm)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        output, hidden = self.gru(embeddings, self.h)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.定义Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = config.ch_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_len = config.max_seq_length\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.attn = nn.Dense(self.hidden_size, self.max_len)\n",
    "        self.softmax = nn.Softmax(axis=2)\n",
    "        self.bmm = P.BatchMatMul()\n",
    "        self.concat = P.Concat(axis=2)\n",
    "        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(axis=2)\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, decoder_input, hidden, encoder_output):\n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.embedding，对输入decoder_input进行Embedding编码。\n",
    "        并对其进行dropout操作\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出命名为embeddings\n",
    "        '''\n",
    "        \n",
    "        out = self.embedding(decoder_input)\n",
    "        embeddings = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        # calculate attn\n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.attn，对embeddings计算注意力权重。\n",
    "        并使用softmax处理注意力权重\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出命名为attn_weights\n",
    "        '''\n",
    "        \n",
    "        out = self.attn(embeddings)\n",
    "        attn_weights = self.softmax(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_output = self.trans(encoder_output, self.perm)\n",
    "        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n",
    "        output =  self.concat((embeddings, attn_applied))\n",
    "        output = self.attn_combine(output)\n",
    "\n",
    "\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, hidden = self.gru(embeddings, hidden)\n",
    "        output = self.cast(output, mstype.float32)\n",
    "        \n",
    "        '''\n",
    "        代码补充\n",
    "        调用self.out，处理上步得到的output，将特征维度映射到词表大小。\n",
    "        并使用self.logsoftmax处理输出\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        将处理得到的输出继续命名为output\n",
    "        '''\n",
    "        output = self.out(output)\n",
    "        output = self.logsoftmax(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.定义Seq2Seq整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, config, is_train=True):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.encoder = Encoder(config, is_train)\n",
    "        self.decoder = Decoder(config, is_train)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.squeeze = P.Squeeze(axis=0)\n",
    "        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n",
    "        self.concat = P.Concat(axis=1)\n",
    "        self.concat2 = P.Concat(axis=0)\n",
    "        self.select = P.Select()\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n",
    "        if self.is_train:\n",
    "            outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_input = dst[::,0:1:1]\n",
    "            decoder_outputs = ()\n",
    "            for i in range(0, self.max_len):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n",
    "                                                                 decoder_hidden, encoder_output)\n",
    "                decoder_hidden = self.squeeze(decoder_hidden)\n",
    "                decoder_output, _ = self.argmax(decoder_output)\n",
    "                decoder_output = self.squeeze(decoder_output)\n",
    "                decoder_outputs += (decoder_output,)\n",
    "                decoder_input = decoder_output\n",
    "            outputs = self.concat(decoder_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(_Loss):\n",
    "    '''\n",
    "       NLLLoss function\n",
    "    '''\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NLLLoss, self).__init__(reduction)\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n",
    "                                     F.scalar_to_array(0.0))\n",
    "        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n",
    "        #print('NLLLoss logits:',logits, logits.shape)\n",
    "        #print('xxx:', logits * label_one_hot)\n",
    "        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n",
    "        return self.get_loss(loss)\n",
    "    \n",
    "class WithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, config):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self.batch_size = config.batch_size\n",
    "        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n",
    "        self._loss_fn = NLLLoss()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.squeeze = P.Squeeze()\n",
    "        self.cast = P.Cast()\n",
    "        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, src, dst, label):\n",
    "        out = self._backbone(src, dst)\n",
    "        loss_total = 0\n",
    "        for i in range(self.batch_size):\n",
    "            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n",
    "                                 self.squeeze(label[i:i+1:1, ::]))\n",
    "            loss_total += loss\n",
    "        loss = loss_total / self.batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.定义训练网络与优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(189235:281473484540480,MainProcess):2022-12-18-14:36:44.747.71 [mindspore/nn/loss/loss.py:166] '_Loss' is deprecated from version 1.3 and will be removed in a future version, use 'LossBase' instead.\n"
     ]
    }
   ],
   "source": [
    "network = Seq2Seq(cfg)\n",
    "network = WithLossCell(network, cfg)\n",
    "optimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\n",
    "model = Model(network, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 定义回调函数，构建模型，启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 125, loss is 2.5734331607818604\n",
      "epoch time: 46065.327 ms, per step time: 368.523 ms\n",
      "epoch: 2 step: 125, loss is 2.2114109992980957\n",
      "epoch time: 9650.846 ms, per step time: 77.207 ms\n",
      "epoch: 3 step: 125, loss is 2.1190006732940674\n",
      "epoch time: 8967.982 ms, per step time: 71.744 ms\n",
      "epoch: 4 step: 125, loss is 1.3826550245285034\n",
      "epoch time: 8925.633 ms, per step time: 71.405 ms\n",
      "epoch: 5 step: 125, loss is 0.9215124249458313\n",
      "epoch time: 8971.074 ms, per step time: 71.769 ms\n",
      "epoch: 6 step: 125, loss is 0.6418973207473755\n",
      "epoch time: 9755.558 ms, per step time: 78.044 ms\n",
      "epoch: 7 step: 125, loss is 0.4848686158657074\n",
      "epoch time: 9011.364 ms, per step time: 72.091 ms\n",
      "epoch: 8 step: 125, loss is 0.3897261321544647\n",
      "epoch time: 9681.094 ms, per step time: 77.449 ms\n",
      "epoch: 9 step: 125, loss is 0.15921345353126526\n",
      "epoch time: 8896.425 ms, per step time: 71.171 ms\n",
      "epoch: 10 step: 125, loss is 0.1928779035806656\n",
      "epoch time: 9073.352 ms, per step time: 72.587 ms\n",
      "epoch: 11 step: 125, loss is 0.152815580368042\n",
      "epoch time: 9259.389 ms, per step time: 74.075 ms\n",
      "epoch: 12 step: 125, loss is 0.2208772748708725\n",
      "epoch time: 9279.477 ms, per step time: 74.236 ms\n",
      "epoch: 13 step: 125, loss is 0.09076489508152008\n",
      "epoch time: 9005.636 ms, per step time: 72.045 ms\n",
      "epoch: 14 step: 125, loss is 0.07416769117116928\n",
      "epoch time: 9182.928 ms, per step time: 73.463 ms\n",
      "epoch: 15 step: 125, loss is 0.10916758328676224\n",
      "epoch time: 8781.099 ms, per step time: 70.249 ms\n",
      "epoch: 16 step: 125, loss is 0.0656392052769661\n",
      "epoch time: 9459.323 ms, per step time: 75.675 ms\n",
      "epoch: 17 step: 125, loss is 0.05078738182783127\n",
      "epoch time: 8745.586 ms, per step time: 69.965 ms\n",
      "epoch: 18 step: 125, loss is 0.13440541923046112\n",
      "epoch time: 8777.712 ms, per step time: 70.222 ms\n",
      "epoch: 19 step: 125, loss is 0.055283769965171814\n",
      "epoch time: 8991.901 ms, per step time: 71.935 ms\n",
      "epoch: 20 step: 125, loss is 0.0702027827501297\n",
      "epoch time: 9002.564 ms, per step time: 72.021 ms\n",
      "epoch: 21 step: 125, loss is 0.008298787288367748\n",
      "epoch time: 8906.537 ms, per step time: 71.252 ms\n",
      "epoch: 22 step: 125, loss is 0.03586554899811745\n",
      "epoch time: 8959.465 ms, per step time: 71.676 ms\n",
      "epoch: 23 step: 125, loss is 0.10448073595762253\n",
      "epoch time: 8835.428 ms, per step time: 70.683 ms\n",
      "epoch: 24 step: 125, loss is 0.10752282291650772\n",
      "epoch time: 9263.508 ms, per step time: 74.108 ms\n",
      "epoch: 25 step: 125, loss is 0.12508754432201385\n",
      "epoch time: 9754.580 ms, per step time: 78.037 ms\n",
      "epoch: 26 step: 125, loss is 0.005018686410039663\n",
      "epoch time: 9010.338 ms, per step time: 72.083 ms\n",
      "epoch: 27 step: 125, loss is 0.019585175439715385\n",
      "epoch time: 9036.466 ms, per step time: 72.292 ms\n",
      "epoch: 28 step: 125, loss is 0.002805812517181039\n",
      "epoch time: 9028.957 ms, per step time: 72.232 ms\n",
      "epoch: 29 step: 125, loss is 0.014614924788475037\n",
      "epoch time: 9139.865 ms, per step time: 73.119 ms\n",
      "epoch: 30 step: 125, loss is 0.031274475157260895\n",
      "epoch time: 8927.792 ms, per step time: 71.422 ms\n",
      "epoch: 31 step: 125, loss is 0.004106024280190468\n",
      "epoch time: 8846.485 ms, per step time: 70.772 ms\n",
      "epoch: 32 step: 125, loss is 0.020930428057909012\n",
      "epoch time: 8901.721 ms, per step time: 71.214 ms\n",
      "epoch: 33 step: 125, loss is 0.012081212364137173\n",
      "epoch time: 8854.509 ms, per step time: 70.836 ms\n",
      "epoch: 34 step: 125, loss is 0.0857691541314125\n",
      "epoch time: 8851.505 ms, per step time: 70.812 ms\n",
      "epoch: 35 step: 125, loss is 0.05350912734866142\n",
      "epoch time: 8862.779 ms, per step time: 70.902 ms\n",
      "epoch: 36 step: 125, loss is 0.002450773026794195\n",
      "epoch time: 8891.497 ms, per step time: 71.132 ms\n",
      "epoch: 37 step: 125, loss is 0.05111050605773926\n",
      "epoch time: 8833.878 ms, per step time: 70.671 ms\n",
      "epoch: 38 step: 125, loss is 0.08757845312356949\n",
      "epoch time: 8927.553 ms, per step time: 71.420 ms\n",
      "epoch: 39 step: 125, loss is 0.06404981017112732\n",
      "epoch time: 8856.506 ms, per step time: 70.852 ms\n",
      "epoch: 40 step: 125, loss is 0.03806254640221596\n",
      "epoch time: 8832.179 ms, per step time: 70.657 ms\n",
      "epoch: 41 step: 125, loss is 0.05748995393514633\n",
      "epoch time: 8919.808 ms, per step time: 71.358 ms\n",
      "epoch: 42 step: 125, loss is 0.03938340023159981\n",
      "epoch time: 8983.893 ms, per step time: 71.871 ms\n",
      "epoch: 43 step: 125, loss is 0.08863779157400131\n",
      "epoch time: 8970.714 ms, per step time: 71.766 ms\n",
      "epoch: 44 step: 125, loss is 0.045860353857278824\n",
      "epoch time: 8953.690 ms, per step time: 71.630 ms\n",
      "epoch: 45 step: 125, loss is 0.07144933938980103\n",
      "epoch time: 8976.901 ms, per step time: 71.815 ms\n",
      "epoch: 46 step: 125, loss is 0.03593174368143082\n",
      "epoch time: 9072.784 ms, per step time: 72.582 ms\n",
      "epoch: 47 step: 125, loss is 0.12550748884677887\n",
      "epoch time: 8822.886 ms, per step time: 70.583 ms\n",
      "epoch: 48 step: 125, loss is 0.04522676765918732\n",
      "epoch time: 8859.460 ms, per step time: 70.876 ms\n",
      "epoch: 49 step: 125, loss is 0.036771804094314575\n",
      "epoch time: 8885.859 ms, per step time: 71.087 ms\n",
      "epoch: 50 step: 125, loss is 0.06321291625499725\n",
      "epoch time: 9024.090 ms, per step time: 72.193 ms\n"
     ]
    }
   ],
   "source": [
    "loss_cb = LossMonitor()\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\n",
    "time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "callbacks = [time_cb, ckpoint_cb, loss_cb]\n",
    "model.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.定义推理网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferCell(nn.Cell):\n",
    "    def __init__(self, network, config):\n",
    "        super(InferCell, self).__init__(auto_prefix=False)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        out = self.network(src, dst)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Seq2Seq(cfg,is_train=False)\n",
    "network = InferCell(network, cfg)\n",
    "network.set_train(False)\n",
    "# 注意，不同的checkpoint请自行设置\n",
    "parameter_dict = load_checkpoint(\"./ckpt/gru_6-50_125.ckpt\")\n",
    "load_param_into_net(network, parameter_dict)\n",
    "model = Model(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.定义翻译测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打开词汇表\n",
    "with open(os.path.join(cfg.dataset_path,\"en_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "en_vocab = list(data.split('\\n'))\n",
    "\n",
    "with open(os.path.join(cfg.dataset_path,\"ch_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "ch_vocab = list(data.split('\\n'))\n",
    "\n",
    "def translate(str_en):\n",
    "    max_seq_len = 10\n",
    "    str_vocab = normalizeString(str_en).split(' ')\n",
    "    print(\"English\",str(str_vocab))\n",
    "    str_id = [1]\n",
    "    for i in str_vocab:\n",
    "        str_id += [en_vocab.index(i)]\n",
    "   \n",
    "    num = max_seq_len + 1 - len(str_id)\n",
    "    if(num >= 0):\n",
    "        str_id += [0]*num\n",
    "    else:\n",
    "        str_id = str_id[:max_seq_len] + [0]\n",
    "    str_id = Tensor(np.array([str_id[1:]]).astype(np.int32))\n",
    "   \n",
    "    out_id = [1]+[0]*10\n",
    "    out_id = Tensor(np.array([out_id[:-1]]).astype(np.int32))\n",
    "    \n",
    "    output = network(str_id, out_id)\n",
    "    out= ''\n",
    "    for x in output[0].asnumpy():\n",
    "        if x == 0:\n",
    "            break\n",
    "        out += ch_vocab[x]\n",
    "    print(\"中文\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.在线推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'love', 'tom']\n",
      "中文 我爱汤姆。\n"
     ]
    }
   ],
   "source": [
    "translate('i love tom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'hate', 'tom']\n",
      "中文 我恨蚊子。\n"
     ]
    }
   ],
   "source": [
    "translate('i hate tom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['hi']\n",
      "中文 嗨。\n"
     ]
    }
   ],
   "source": [
    "translate('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'want', 'a', 'phone']\n",
      "中文 我想要一张邮票。\n"
     ]
    }
   ],
   "source": [
    "translate('i want a phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'want', 'a', 'dog']\n",
      "中文 我想要一只狗。\n"
     ]
    }
   ],
   "source": [
    "translate('i want a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['he', 'is', 'a', 'good', 'boy']\n",
      "中文 他是个好人。\n"
     ]
    }
   ],
   "source": [
    "translate('he is a good boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['he', 'is', 'a', 'bad', 'boy']\n",
      "中文 他是个胖子\n"
     ]
    }
   ],
   "source": [
    "translate('he is a bad boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'will', 'do', 'my', 'best']\n",
      "中文 我会尽力而为。\n"
     ]
    }
   ],
   "source": [
    "translate('i will do my best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'would', 'like', 'to', 'eat']\n",
      "中文 我会在这里吃饭。\n"
     ]
    }
   ],
   "source": [
    "translate(\"i would like to eat \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['it', 'is', 'very', 'sad']\n",
      "中文 这很难过。\n"
     ]
    }
   ],
   "source": [
    "translate('it is very sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'love', 'you']\n",
      "中文 我爱您。\n"
     ]
    }
   ],
   "source": [
    "translate('i love you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
